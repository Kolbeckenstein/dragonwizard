"""
Data models for the LLM Orchestration Layer.

These Pydantic models define the structured output of LLM interactions,
following the same pattern as dragonwizard.rag.base for consistency.

Key design decisions:
- LLMResponse captures everything needed by downstream consumers (Discord bot):
  the answer text, any tool calls that occurred, which model answered, and
  token usage for cost tracking.
- ToolCall records are kept for transparency — the Discord layer can show
  users what dice were rolled, what lookups happened, etc.
- TokenUsage tracks prompt + completion tokens so we can monitor API costs.
- LLMError wraps provider-specific exceptions into a single type that
  callers can catch without knowing whether we're using Anthropic, OpenAI, etc.
"""

from typing import Any

from pydantic import BaseModel, Field, computed_field


class TokenUsage(BaseModel):
    """Token consumption for a single LLM API call (or accumulated across a tool loop)."""

    prompt_tokens: int = Field(ge=0, description="Tokens in the prompt (system + messages + tools)")
    completion_tokens: int = Field(ge=0, description="Tokens generated by the model")

    @computed_field
    @property
    def total_tokens(self) -> int:
        """Total tokens consumed (prompt + completion)."""
        return self.prompt_tokens + self.completion_tokens


class ToolCall(BaseModel):
    """
    Record of a single tool invocation during response generation.

    Stored on LLMResponse so downstream layers (Discord bot) can display
    tool activity to users — e.g., "I rolled 2d6+3 and got 12."
    """

    name: str = Field(description="Tool name (e.g., 'roll_dice')")
    arguments: dict[str, Any] = Field(description="Arguments passed to the tool")
    result: str = Field(description="Text result returned by the tool")


class LLMResponse(BaseModel):
    """
    Structured output from a single generate_response() call.

    This is what the Discord bot layer will consume to build user-facing messages.
    """

    text: str = Field(description="The final answer text from the LLM")
    tool_calls: list[ToolCall] = Field(
        default_factory=list,
        description="Ordered record of all tool calls made during generation",
    )
    model: str = Field(description="Model identifier that produced the response")
    usage: TokenUsage = Field(description="Token usage across all API calls in this generation")


class LLMError(Exception):
    """
    Wrapper for LLM provider errors.

    Raised when the LLM API call fails (rate limits, auth errors, network issues, etc.).
    Wraps the provider-specific exception so callers only need to catch one type,
    regardless of whether we're using Anthropic, OpenAI, or a local model via LiteLLM.
    """

    def __init__(self, message: str, cause: Exception | None = None):
        super().__init__(message)
        self.__cause__ = cause
